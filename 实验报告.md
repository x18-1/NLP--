# 新闻分类

## 摘要

使用selenium进行爬取到了澎湃新闻的5个分类下的数据。并将该分类作为标签进行新闻分类。基于字典树的停用词处理解决了用循环处理慢的问题。使用jieba进行分词。构造朴素贝叶斯、支持向量机、LightGBM等机器学习模型。模型准确度达到0.84左右。进行折交叉验证提高了模型的精度，达到0.86左右，但模型有较严重过拟合，降低过拟合的同时会降低精度。使用深度学习模型建模BiLSTM，模型精度也在0.82左右，使用预训练模型Word2vec作为Embedding层，模型精度达到0.87左右，使用深度学习TextCNN+Word2vec进一步提高模型精度，达到了0.90左右，但模型依然存在着一定的过拟合。使用BERT大模型是模型精度达到0.88左右，同时提高了模型的泛化能力。主要模型分数如下：

|               模型               | 测试集最好分数 |
| :------------------------------: | :------------: |
| 朴素贝叶斯（交叉验证的词袋表示） |     0.861      |
|     支持向量机（折交叉验证）     |     0.875      |
|   BiLSTM（预训练模型+warm-up）   |     0.885      |
|      TextCNN（预训练模型）       |     0.903      |
|         BERT（warm-up）          |      0.88      |

总结而言BERT的效果最好，因为在保证分数的情况下减少的过拟合，极大的提高了泛化能力。TextCNN也有着不错的能力。朴素贝叶斯再计算量很低的情况下也能达到不错的分数。



关键字：selenium	字典树	jieba	hanlp	朴素贝叶斯	SVM	LightGBM	折交叉验证	BiLSTM	TextCNN	BERT	warm-up	word2vec	Tfidf	词袋	PCA

## 背景介绍

1. 新闻分类的目的：新闻分类的目的是将新闻分成不同的类别，以便更好地管理和组织新闻信息。这样，人们就可以更快地找到感兴趣的新闻，并且可以更方便地浏览不同类别的新闻。
2. 新闻分类的意义：新闻分类的意义在于，它能帮助人们快速了解不同类别的新闻内容，并且能帮助新闻网站或应用更好地组织和推荐新闻。此外，新闻分类还可以帮助更好地理解新闻的发展趋势和聚焦的焦点。
3. 新闻分类的应用：新闻分类的应用涉及到很多领域，包括新闻网站和应用、新闻订阅服务、搜索引擎和其他信息提供服务。新闻分类的应用还可以扩展到其他相关领域，如市场营销、广告投放和公共关系等。

## 数据获取

本次主题，我们爬取的是澎湃新闻中新闻标题，总共爬取了5个部分，每个部分分别有1000多条数据，五个部分分别是财经、国际、科技、时事、思想。

爬取流程：

1. 使用selenium进行模拟网页行为，通过`js`代码是网页往下滑动加载数据
2. 打印网页源码，利用`lxml`库中`etree`进行解析

爬取过程中出现的问题及解决方案：

1. 问题一:在爬取页面中遇到了新闻以加载的方式呈现。最后通过`selenium`+`javascript`模拟滑动进行解决。
2. 问题二:在爬取到最后，发现爬出的是空集。之后通过查看源代码，发现由于使用的是谷歌驱动，同时Edge和谷歌在同一位置的Xpath不同，最后修改`Xpath`解决问题。

爬取结果如下：

![image-20221221191202137](https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221221191202137.png)

最终整合成`train.csv`

## 数据预处理

1. 观察数据，发现再部分句子末尾存在着形如：2022-12-0915、2022-12-09、3天前、22小时前的词。使用正则表达式将其去除，核心代码如下：

   ```python
   def f(x):
     # 删除形如：2022-12-0915、2022-12-09
     pattern = r"\d{1,9}-\d{1,9}-\d{1,9}"
     result = re.sub(pattern, "", x)
     return result
   def f1(x):
       text = re.sub(r"\d+小时前\d+", "", x)
       return text
   def f2(x):
       text = re.sub(r"\d+天前\d+", "", x)
       return text
   ```

2. 使用字典树进行停用词过滤。字典树，是一种树形结构，典型应用是用于统计，排序和保存大量的字符串。主要思想是利用字符串的**公共前缀**来节约存储空间，字典树对于中文的检索比英文更慢，因为词与词之间的公共前缀可能会少。但经过测试也会比循环查找快许多。

   <img src="https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222181120514.png" alt="image-20221222181120514" style="zoom: 33%;" />

   核心代码如下：

   ```python
   def Dropwords(drop_words,sentence_array):
       t = Trie.Trie()
       for i in drop_words:
           t.insert(i)
       contents_clean = []
       all_words = []
       for line in sentence_array:
           line_clean = []
           for word in line:
               if t.search(word):			# 高效查找
                   continue
               line_clean.append(word)
               all_words.append(str(word))
           contents_clean.append(line_clean)
       del t								# 删除字典树
       return contents_clean,all_words
   ```

3. 分词。本次使用的分词方式有两种：`jieba`、`MSR_TOK_ELECTRA_BASE_CRF`（hanlp库中的一个模型）。两者效果差不多，且经过测试jieba会更好一点点。同时jieba分词会更快。但经过比较了几十条句子，另一个分词分出的结果要更好，更加符合我们的认知。

   ```python
   def cut_word(x,method='hanlp'):
       if method == 'jieba':
           return [jieba.lcut(i) for i in x]
       elif method == 'hanlp':
           tok = hanlp.load(hanlp.pretrained.tok.MSR_TOK_ELECTRA_BASE_CRF) 
           return tok(x)
   ```

4. 机器学习数据处理

   + TFidf：使用sklearn库的`TfidfVectorizer`将文本转换为向量

   + 词袋表示：假设文本中的词语是没有顺序的集合，将文本中的全部词所对应的向量表示（既可以是独热表示，也可以是分布式表示或词向量）相加，即构成了文本的向量表示，使用sklearn的`CountVectorizer`将文本转换为向量

   + 预训练模型法：使用预训练模型将每一句话对应词的词向量获取，由于句子长度不一，因此进行加和求平均。求平均操作会损失很多信息，因此带来的效果也并没太好。部分代码如下：

     ```python
     def Text2Vec(x,method='word2vec'):
         if method=='word2vec':
             model = hanlp.load(hanlp.pretrained.word2vec.MERGE_SGNS_BIGRAM_CHAR_300_ZH)
             res = []
             for i in x:
                 res.append([model(j).cpu() for j in i])
             res = [np.sum(i)/len(i) for i in res]               # 因为句子的长度不一，在这里进行取个均值的操作（损失信息）。如何可以的话可以进行拼接tensor
             return res
     ```

5. 深度学习数据处理：

   + 词典映射：使用Vocab将每一个词（根据词频）映射成`id`，即`token_to_idx`。这也是NLP自然处理最基础的一步

   + 词的填充与截取：由于词的长度不一，如图，句子长度主要集中再5-20之间。因此对于较短的句子进行填充词元`"<pad>"`，较长的句子进行截断：

     <img src="https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222182932618.png" alt="image-20221222182932618" style="zoom:50%;" />

     ```python
     def truncate_pad(line, num_steps, padding_token):
         if len(line) > num_steps:
             return line[:num_steps]  # 截取
         return line + [padding_token] * (num_steps - len(line))  # 填充
     ```

     在进行BiLSTM也采用了一种每个批量根据最长的句子进行填充的方法。

   + 词嵌入表示

     + 使用`nn.Embedding`来表示词向量，在训练过程中进行动态调整。

     + 使用预训练模型`Word2vec`来代替Embedding层，会给模型带来较大的效果提升。核心代码如下：

       ```python
       def TokenEmbedding(x,method='word2vec'):	# x时idx_to_token
           if method=='word2vec':
               model = hanlp.load(hanlp.pretrained.word2vec.MERGE_SGNS_BIGRAM_CHAR_300_ZH)
               return model(x)
           elif method == 'fasttxt':
               model = hanlp.load(hanlp.pretrained.fasttext.FASTTEXT_CC_300_EN)
               return model(x)
       ```
     
   + 使用BERT对句子进行编码；本次使用是：序列分类（对应的还有`Token分类`），也就是对一个句子直接进行编码。
   
     ```python
     def collate_fn(data):
         inputs = [i[0] for i in data]
         labels = [i[1] for i in data]
     
         # 编码
         data = token.batch_encode_plus(
             batch_text_or_text_pairs = inputs,
             truncation = True,
             padding="max_length",
             return_tensors = "pt",
             return_length = True,
         )
     
         input_ids = data['input_ids']
         attention_mask = data['attention_mask']
         token_type_ids = data['token_type_ids']
         labels = torch.LongTensor(labels)       # nlp中常用LongTensor，64位浮点型
         return input_ids, attention_mask, token_type_ids, labels
     ```
   
     

## 模型建立与检验

### 1.机器学习

机器学习代码统一使用一个训练函数：

```python
def train(x_train,y_train,x_test,y_test,model,is_showCM=False):
    model.fit(x_train,y_train)
    train_pred = np.argmax(model.predict_proba(x_train),axis=1)
    test_pred = np.argmax(model.predict_proba(x_test),axis=1)
    print("训练集的准确度:",acc(y_train,train_pred))
    print("测试集的准确度:",acc(y_test,test_pred))

    if is_showCM:
        train_cm = confusion_matrix(y_train,train_pred)
        train_cm = pd.DataFrame(train_cm,columns=encoder.classes_,index=encoder.classes_)
        test_cm = confusion_matrix(y_test,test_pred)
        test_cm = pd.DataFrame(test_cm,columns=encoder.classes_,index=encoder.classes_)
        fig, ax = plt.subplots(1,2, tight_layout = True, figsize = (15,5))
        
        sns.heatmap(train_cm, annot=True,fmt='.0f' ,cbar = False,ax=ax[0])
        sns.heatmap(test_cm, annot=True,fmt='.0f' ,cbar = False,ax=ax[1])
        ax[0].set_title('Train Confusion Matrix')
        ax[1].set_title('Test Confusion Matrix')
        ax[0].set_ylabel('Actual Values')
        ax[0].set_xlabel('Predicted Values')
        ax[1].set_xlabel('Predicted Values')
        sns.despine()
```

1. 朴素贝叶斯：

   模型的建立：

   ```python
   NB = MultinomialNB()
   ```

   结果展示：

   ![image-20221222213825327](https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222213825327.png)

   模型精度再0.84左右，存在着过拟合。

   ![image-20221222213859821](https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222213859821.png)

   这是词袋表示法的训练集和测试集混淆矩阵，可以看出科技这个类别分的不是很好

   为了提高结果，使用**折交叉验证**进行训练，最终结果如下：

   ![image-20221222214016662](https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222214016662.png)

   可以看到测试集分数提高些许，但还是存在着过拟合。

   我们还尝试过使用**PCA**对数据进行降维，因为数据是稀疏的。但朴素贝叶斯不接受负数作为输入，尝试进行归一化再送入，但效果很差

2. 支持向量机

   支持向量机在进行折交叉验证和不进行折交叉验证的区别不大，下面是进行折交叉验证达到的分数

   ![image-20221223123034798](https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221223123034798.png)

   也存在着一定的过拟合。

3. LightGBM

   LightGBM是一种集成模型，树模型的一种，可以算得上是竞赛大杀器。但树模型在这里表现的结果极差。巨大的维度使的树模型不能很好的进行特征提取。同时将降维后的数据进行训练，速度会变得极慢，而且准确度也不高。



### 2.深度学习

1. BiLSTM模型

   BiLSTM全称为Bi-directional Long Short-Term Memory，是双向循环长短期记忆神经网络。

   本次采用的模型架构：

   <img src="https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222195650143.png" alt="image-20221222195650143" style="zoom: 67%;" />

   网络模型有：

   ```python
   class BiRNN(nn.Module):
       def __init__(self, vocab_size, embed_size, num_hiddens,
                    num_layers, **kwargs):
           super(BiRNN, self).__init__(**kwargs)
           self.embedding = nn.Embedding(vocab_size, embed_size)
           # 将bidirectional设置为True以获取双向循环神经网络
           self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,
                                   bidirectional=True)
           self.decoder = nn.Sequential(
               nn.Linear(4 * num_hiddens, len(encoder.classes_)),
           )
       def forward(self, inputs):
           embeddings = self.embedding(inputs.T)
           self.encoder.flatten_parameters()           
           outputs, _ = self.encoder(embeddings)
           encoding = torch.cat((outputs[0], outputs[-1]), dim=1)
           outs = self.decoder(encoding)
           return outs
   ```

   第一次采用进行RNN的时候，Embedding采用原生Embedding层，希望在训练的过程中不断进行调整。效果不好，精度只有0.81左右。

   第二次通过对Word2vec对vocab里面的词元进行编码形成`vocab_size,300`（300是word2vec词向量的维度）的矩阵，将此矩阵代替作为Embedding层，并设置其训练过程中参数不更新。最终效果如下：

   ![image-20221222200447825](https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222200447825.png)

   `train acc 1.000, test acc 0.871`，可以看到准确度提高了不少，但是模型存在着过拟合。

   第三次采用了另一种处理方法，对每一个批量的数据进行填充，使每个小批量里具有相同的长度的词元。网络架构如图所示：

   ```python
   class LSTM(nn.Module):
       def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):
           super(LSTM, self).__init__()
           self.embeddings = nn.Embedding(vocab_size, embedding_dim)                   # 词向量
           self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,batch_first=True,bidirectional=True)            # LSTM
           self.output = nn.Sequential(
               nn.Linear(hidden_dim, 300*4),                              # 线性输出层
               nn.Dropout(0.2),
               nn.Linear(1200,512),
               nn.Linear(512,num_class)
           )
       def forward(self, inputs, lengths):									# 带有length的输入
           embeddings = self.embeddings(inputs)    
           x_pack = pack_padded_sequence(embeddings, lengths.to('cpu'), batch_first=True, enforce_sorted=False)            # length要放到cpu上
           hidden, (hn, cn) = self.lstm(x_pack)
           outputs = self.output(hn[-1])
           return outputs
   ```

   在训练过程中发现**loss抖动的很大**，采用一种**warm-up**的训练方法，每经过n个epoch，学习率降低一些。最终结果如下图

   <img src="https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222203301441.png" alt="image-20221222203301441" style="zoom:67%;" />

   `| train acc 1.000 | test acc 0.885 |`训练集的准确度又提高一点。不过仍存在一定的过拟合。

2. TextCNN

   使用卷积神经网络对序列化数据进行建模。使用一维卷积再序列上进行扫描进行特征提取。网络架构如图所示：

   <img src="https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222194952574.png" alt="image-20221222194952574" style="zoom: 67%;" />

   在进行建模的时候使用两个Embedding层，一个Embedding层进行微调，一个不会变化（不会进行参数更新），两者都是copy的Word2vec映射之后的词向量。实验证明两个微调Embedding或者是两个常量Embedding或者是单个Embedding的效果都不如以上架构效果好。两者进行拼接进入四个卷积层，四个卷积层的卷积核和输出通道数分别为：[2, 3, 4, 5], [400, 300, 300, 200]。最后经过最大时间汇聚连接起来，在这里尝试过平均时间汇聚，损失过多的信息，效果很差。接下来经过激活函数和Dropout进行解码层，也就是一个感知机。

   网络模型如下：

   ```python
   class TextCNN(nn.Module):
       def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,
                    **kwargs):
           super(TextCNN, self).__init__(**kwargs)
           self.embedding = nn.Embedding(vocab_size, embed_size)
           self.constant_embedding = nn.Embedding(vocab_size, embed_size)
           self.dropout = nn.Dropout(0.5)
           self.pool = nn.AdaptiveMaxPool1d(1)         
           self.relu = nn.ReLU()
           self.convs = nn.ModuleList()
           for c, k in zip(num_channels, kernel_sizes):
               self.convs.append(nn.Conv1d(2 * embed_size, c, k))      # 3*embed_size 隐藏层大小
               
           self.decoder = nn.Sequential(
               nn.Linear(sum(num_channels), 512),
               nn.Linear(512,len(encoder.classes_))
           )
   
       def forward(self, inputs):
           embeddings = torch.cat((
               self.embedding(inputs), self.constant_embedding(inputs)), dim=2)
           embeddings = embeddings.permute(0, 2, 1)
           encoding = torch.cat([
               torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)
               for conv in self.convs], dim=1)
           outputs = self.decoder(self.dropout(encoding))              # 闭包？柯里化？
           return outputs
   ```

   最终结果如下：

   ![image-20221222204252542](https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222204252542.png)

   `train acc 1.000, test acc 0.903`分数又提高到了0.90。但还是存在着一些过拟合。

   再TextCNN中也尝试过用`warm-up`不过效果不如RNN的好。

3. BERT

   2018年的10月11日，Google发布的论文《Pre-training of Deep Bidirectional Transformers for Language Understanding》，成功在 11 项 NLP 任务中取得 state of the art 的结果，赢得自然语言处理学界的一片赞誉之声。

   使用的python的库是`transformer`，使用的预训练模型是：`bert-base-chinses`，一个比下载量比较高预训练模型。由于BERT太过复杂网络架构不再给出，只是再Bert模型的基础上加了一个线性输出层。

   ```python
   class Model(torch.nn.Module):
       def __init__(self):
           super().__init__()
           self.fc = torch.nn.Linear(768,len(encoder.classes_))
       
       def forward(self,input_ids,attention_mask,token_type_ids):
           # 特征抽取
           with torch.no_grad():
               out = pretrained(						# pretrained：预训练代码	
                   input_ids = input_ids,
                   attention_mask = attention_mask,
                   token_type_ids = token_type_ids
               )
           out = self.fc(out.last_hidden_state[:,0])   # 只需要取第0个词进行分类就可以了，这与bert的设计有关
           return out
   ```

   最终也是达到了一个较好的效果：测试集分数达到了0.88。并且提高了模型的泛化能力，模型不在过拟合。

   <img src="https://xingqiu-tuchuang-1256524210.cos.ap-shanghai.myqcloud.com/9147/image-20221222210428176.png" alt="image-20221222210428176" style="zoom:67%;" />

## 不足与改进

1. 本次使用的Bert模型是`bert-base-chinese`，具有极高的时间复杂度。在本次测试中跑10epoch数据再`goole colab`的T4GPU上需要跑30分钟。本机RTX1650需要跑1小时左右。可以进行进行模型蒸馏来压缩模型或者是采用其他大模型。
2. Bert模型没后只是简单加了一个线性激活层，可以尝试与CNN和LSTM模型进行组合
3. 机器学习模型可以进行大量调参和模型融合
4. 这次过多的专注于模型，可以采取更多数据预处理的方法。

## 完整代码

完整代码过多，无法展示。现已将其放到Github上：

https://github.com/x18-1/NLP-NewsClassification

可以尝试Gittee:

https://gitee.com/xcodd/NLP-NewsClassification



以下是部分代码片段：

```python
def pre_data(x_train_,x_test_,n = 0.95,method='count',is_pca=True):
	# 机器学习数据处理
    if method == "count":
        COUNT = CountVectorizer(analyzer='word')
        x_train_ = COUNT.fit_transform(x_train_).toarray()        # 转换训练集
        x_test_ = COUNT.transform(x_test_).toarray()             # 假设测试集与训练集同分布，进行Transformer转换
        pca = PCA(n_components=n)                # 保留95%的信息
        if is_pca:
            x_train_ = pca.fit_transform(x_train_)
            x_test_ = pca.transform(x_test_)
            return x_train_,x_test_   # 假设同分布，先进行fit
        return x_train_,x_test_   
    else:
        TFIDF = TfidfVectorizer(analyzer='word')
        x_train_ = TFIDF.fit_transform(x_train_).toarray()        # 转换训练集
        x_test_ = TFIDF.transform(x_test_).toarray()              # 假设测试集与训练集同分布，进行Transformer转换
        pca = PCA(n_components=n)                # 保留95%的信息
        if is_pca:
            x_train_ = pca.fit_transform(x_train_)
            x_test_ = pca.transform(x_test_)
            return x_train_,x_test_   # 假设同分布，先进行fit
        return x_train_,x_test_   
```

```python
def train_with_length(net, train_iter, test_iter, loss, optimizer, num_epochs,
               device,scheduler=None):
	# 神经网络代码训练，带有length的
    train_acc_list = []
    test_acc_list = []
    train_loss_list = []
    num_batches = len(train_iter)

    net.to(device)
    for epoch in range(num_epochs):
        print("-"*65)
        metric = Accumulator(4)											# 累加类
        for i, (features, length, labels) in enumerate(train_iter):
            l, acc = train_batch_with_length(							
                net, features, length, labels, loss, optimizer, device)
            metric.add(l, acc, labels.shape[0], labels.numel())
            
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:			#  输出训练信息
                
                print(f"| epoch{epoch:3d} | avg_loss:{metric[0] / metric[2]:.3f} | train_acc:{metric[1] / metric[3]:.2f} |")
                
        if scheduler:
            scheduler.step()                                # 进行warm up
        print('-'*65)      
        test_acc = evaluate_acc(net, test_iter)				# 推理：评估测试集
        print(f"| epoch{epoch:3d} | avg_loss:{metric[0] / metric[2]:.3f} | train_acc:{metric[1] / metric[3]:.2f} | test_acc:{test_acc:.2f}| lr:{optimizer.param_groups[0]['lr']} | ")
        train_acc_list.append(metric[1] / metric[3])			# 添加到列表
        train_loss_list.append(metric[0] / metric[2])			
        test_acc_list.append(test_acc)
        
    print("-"*89)   
    print("final metris:")
    print(f'| avg_loss {metric[0] / metric[2]:.3f} | train acc '
          f'{metric[1] / metric[3]:.3f} | test acc {test_acc:.3f} |')
    train_acc_list.append(metric[1] / metric[3])
    train_loss_list.append(metric[0] / metric[2])
    test_acc_list.append(test_acc)
    plt.plot(train_acc_list,label="train_acc")				# 绘图
    plt.plot(train_loss_list,label="train_loss")
    plt.plot(test_acc_list,label="test_acc")
    plt.legend()
    plt.show()
```

```python
def load_data(
    batch_size,
    num_steps=25,
    min_fred = 0,
    path="../Get_Data/train2.csv",
    collate_fn=collate_fn,
    method = "hanlp",
    is_truncate_pad=True,
    is_dropword=False,
    is_gotchinese=False ):
	# 深度学习数据加载
    from sklearn.preprocessing import LabelEncoder
    from sklearn.model_selection import train_test_split

    data = pd.read_csv(path)
    if is_gotchinese:
        data['title'] = data['title'].map(got_chinese)     # 取出标点符号
    cut_words = cut_word(data['title'].to_list(),method=method)  # 分词
    stopwords =  pd.read_csv("../Get_data/hit_stopwords.txt",index_col=False,sep="\t",quoting=3,names=['stopword'], encoding='utf-8').stopword.values.tolist() #list
    cut_words = Dropwords(stopwords,cut_words)[0] if is_dropword else cut_words		# 停用词过滤
    vocab_ = vocab.Vocab(cut_words,reserved_tokens=['<pad>'],min_fred=min_fred)		# 词典映射
    X = [truncate_pad(vocab_[i],num_steps,vocab_['<pad>']) for i in cut_words] if is_truncate_pad else [vocab_[i] for i in cut_words]				# 是否填充或接单
    y = data['label']
    encoder = LabelEncoder()
    y = encoder.fit_transform(y)			# 标签映射
    x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=0.2 ,shuffle=True ,random_state=1,stratify=y)				# 分割数据集
    train_dataset = Dataset(list(zip(x_train,y_train)))			# 深度学习中常用的数据加载类
    test_dataset = Dataset(list(zip(x_test,y_test)))
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)								# 深度学习常用dataloader类
    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)
    return train_data_loader,test_data_loader,vocab_,encoder
```











